---
layout: post
title: 简单爬虫学习笔记（一）
author: wzhzzmzzy
date: 2017-09-24
categories: Tech
tags: 
    - Python
    - 计算机网络
description: 从头再学一遍Python爬虫，把学到的东西都记录一下。
---

### 学习资料

主要的资料整理[@路人甲](https://www.zhihu.com/people/sgai/)都已经做好整理了，在这里：[一份Python爬虫电子书](https://zhuanlan.zhihu.com/p/28865834)。从基础的Python入门，到Python爬虫如何学习，都有。如果能学完一遍，那么简单的网页爬虫应该没什么问题了。

顺便推荐一本《Python网络数据采集》，其中以Wikipedia为例解说爬虫的工作，写得不错。

### 第一天 —— 糗事百科热点爬虫

#### 成品

这是最简单的一个爬虫，因为糗百几乎没有做任何反爬工作，非常的轻松。Github地址：[糗事百科爬虫](https://github.com/wzhzzmzzy/Python-Spider-Workbook/tree/master/01%20qiubai)。

#### 思路

首先明确一下目标，要把糗百热点页面上的每一条消息都存下来，放进一个文本文件里。

那么第一步，先确定网址，`https://www.qiushibaike.com/hot/page/`。这是默认的第一页，往后翻页就会变成`/page/2/`。那么我们可以用一个参数指定需要爬几个页面。

第二步，尝试获取一下HTML：

```python
request = urllib2.Request(url, headers={'User-Agent': 'Chrome/61.0.3163.91'})
response = urllib2.urlopen(request)
pageCode = response.read().decode('utf-8')
print pageCode
```

哇，这个HTML有点迷。我们会发现，这个网页上有些太长的段子会被去掉最后一段，换成`...`。那么我们要获取完整的段子，就需要进入到更深的页面里。

我们看一下HTML里的`<a>`标签：

```html
<a href="/article/119564633" target="_blank" class="contentHerf" onclick="_hmt.push(['_trackEvent','web-list-content','chick'])">
...
</a>
```

没错，我们要的就是这个`/article/119564633`！那么我们用一个正则表达式来获取一下：

```python
self.urlPattern = re.compile('<div class="articleGender womenIcon">\d*</div>.*?</div>.*?<a href="(.*?)".*?<div class="content">', re.S)
```

这样我们就拿到了这个后缀。在这个后缀页面里面，我们能获取到作者、头像和内容，不过为了简化，没有获取头像。用BeautifulSoup也能实现相应的筛选，不过我选择了正则表达式。

这样一个糗百爬虫就写完了，非常简单。

### 第二天 —— 百度贴吧帖子爬虫

#### 成品

百度贴吧爬虫也是随便写的，没有写太多注释和异常处理。Github地址：[百度贴吧爬虫](https://github.com/wzhzzmzzy/Python-Spider-Workbook/tree/master/02%20tieba)。

#### 思路

贴吧页面明显比糗百复杂一点，不过确实差别不大。所以我加入了一个下载帖子中图片的功能。